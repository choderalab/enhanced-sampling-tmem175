Sender: LSF System <lsfadmin@ln05>
Subject: Job 9377104: <#!/usr/bin/env bash;# Set walltime limit;#BSUB -W 24:00; # Set output file;#BSUB -o  %J.out; #BUSB -J "open_to_closed"; # Set error file;#BSUB -e %J.stderr; # Specify node group;#BSUB -q gpuqueue;#BSUB -R select[gpu_model0!='NVIDIAGeForceGTX1080'];#BSUB -R select[gpu_model0!='NVIDIAGeForceGTX1080Ti'];  # nodes: number of nodes and GPU request;#BSUB -n 1 -R "rusage[mem=8]";#BSUB -gpu "num=1:j_exclusive=yes:mode=shared";  source ~/.bashrc;conda activate enhanced-sampling-tmem175;python ../pulling_methods/pulling_sequential.py -i ../systems/system00 -e ../systems/eBDims/open_to_closed_superposed_to_final.pdb -o ../../enhanced-sampling-tmem175-data/pulling/01.open_to_closed -p ../sim_params/pulling.yaml -y ../pulling_methods/cas.yaml> in cluster <lila> Done

Job <#!/usr/bin/env bash;# Set walltime limit;#BSUB -W 24:00; # Set output file;#BSUB -o  %J.out; #BUSB -J "open_to_closed"; # Set error file;#BSUB -e %J.stderr; # Specify node group;#BSUB -q gpuqueue;#BSUB -R select[gpu_model0!='NVIDIAGeForceGTX1080'];#BSUB -R select[gpu_model0!='NVIDIAGeForceGTX1080Ti'];  # nodes: number of nodes and GPU request;#BSUB -n 1 -R "rusage[mem=8]";#BSUB -gpu "num=1:j_exclusive=yes:mode=shared";  source ~/.bashrc;conda activate enhanced-sampling-tmem175;python ../pulling_methods/pulling_sequential.py -i ../systems/system00 -e ../systems/eBDims/open_to_closed_superposed_to_final.pdb -o ../../enhanced-sampling-tmem175-data/pulling/01.open_to_closed -p ../sim_params/pulling.yaml -y ../pulling_methods/cas.yaml> was submitted from host <lilac-ln02> by user <paynea> in cluster <lila> at Thu Sep 22 10:50:03 2022
Job was executed on host(s) <ln05>, in queue <gpuqueue>, as user <paynea> in cluster <lila> at Thu Sep 22 10:50:04 2022
</home/paynea> was used as the home directory.
</data/chodera/paynea/enhanced-sampling-tmem175/hpc_scripts> was used as the working directory.
Started at Thu Sep 22 10:50:04 2022
Terminated at Fri Sep 23 00:53:32 2022
Results reported at Fri Sep 23 00:53:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash
# Set walltime limit
#BSUB -W 24:00

# Set output file
#BSUB -o  %J.out

#BUSB -J "open_to_closed"

# Set error file
#BSUB -e %J.stderr

# Specify node group
#BSUB -q gpuqueue
#BSUB -R select[gpu_model0!='NVIDIAGeForceGTX1080']
#BSUB -R select[gpu_model0!='NVIDIAGeForceGTX1080Ti']


# nodes: number of nodes and GPU request
#BSUB -n 1 -R "rusage[mem=8]"
#BSUB -gpu "num=1:j_exclusive=yes:mode=shared"


source ~/.bashrc
conda activate enhanced-sampling-tmem175
python ../pulling_methods/pulling_sequential.py -i ../systems/system00 -e ../systems/eBDims/open_to_closed_superposed_to_final.pdb -o ../../enhanced-sampling-tmem175-data/pulling/01.open_to_closed -p ../sim_params/pulling.yaml -y ../pulling_methods/cas.yaml

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   48743.52 sec.
    Max Memory :                                 5 GB
    Average Memory :                             3.43 GB
    Total Requested Memory :                     8.00 GB
    Delta Memory :                               3.00 GB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                657
    Run time :                                   50610 sec.
    Turnaround time :                            50609 sec.

The output (if any) follows:



PS:

Read file <9377104.stderr> for stderr output of this job.

